{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google Cloud Vision API Demo - 1 Jun 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaHV4Kb2Xo0L"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vrJtXv7TUiv"
      },
      "source": [
        "!pip3 install -U pip google-cloud-vision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NppprDcnTUiy"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAHZ9sDvTUi1"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG97Br8_lwS5"
      },
      "source": [
        "import os, io\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'service-account.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jYlsL3Bm-qv"
      },
      "source": [
        "## Label Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ozp2b57TUi5"
      },
      "source": [
        "from __future__ import print_function\n",
        "from google.cloud import vision\n",
        "\n",
        "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "image = vision.Image()\n",
        "image.source.image_uri = image_uri\n",
        "\n",
        "response = client.label_detection(image=image)\n",
        "\n",
        "print('Labels (and confidence score):')\n",
        "print('=' * 30)\n",
        "for label in response.label_annotations:\n",
        "    print(label.description, '(%.2f%%)' % (label.score*100.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYkSzeFuyDfJ"
      },
      "source": [
        "## Text Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzAeKkTEn21W"
      },
      "source": [
        "from __future__ import print_function\n",
        "from google.cloud import vision\n",
        "\n",
        "image_uri = 'gs://cloud-vision-codelab/otter_crossing.jpg'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "image = vision.Image()\n",
        "image.source.image_uri = image_uri\n",
        "\n",
        "response = client.text_detection(image=image)\n",
        "\n",
        "for text in response.text_annotations:\n",
        "    print('=' * 30)\n",
        "    print(text.description)\n",
        "    vertices = ['(%s,%s)' % (v.x, v.y) for v in text.bounding_poly.vertices]\n",
        "    print('bounds:', \",\".join(vertices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waOOpWqaTu5u"
      },
      "source": [
        "## Landmark Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08FOYVTicvyN"
      },
      "source": [
        "from __future__ import print_function\n",
        "from google.cloud import vision\n",
        "\n",
        "image_uri = 'gs://cloud-vision-codelab/eiffel_tower.jpg'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "image = vision.Image()\n",
        "image.source.image_uri = image_uri\n",
        "\n",
        "response = client.landmark_detection(image=image)\n",
        "\n",
        "for landmark in response.landmark_annotations:\n",
        "    print('=' * 30)\n",
        "    print(landmark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5nG9YlRnpzL"
      },
      "source": [
        "## Emotional Face Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1GzQU1vELIM"
      },
      "source": [
        "from __future__ import print_function\n",
        "from google.cloud import vision\n",
        "\n",
        "uri_base = 'gs://cloud-vision-codelab'\n",
        "pics = ('face_surprise.jpg', 'face_no_surprise.png')\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "image = vision.Image()\n",
        "\n",
        "for pic in pics:\n",
        "    image.source.image_uri = '%s/%s' % (uri_base, pic)\n",
        "    response = client.face_detection(image=image)\n",
        "\n",
        "    print('=' * 30)\n",
        "    print('File:', pic)\n",
        "    for face in response.face_annotations:\n",
        "        likelihood = vision.Likelihood(face.surprise_likelihood)\n",
        "        vertices = ['(%s,%s)' % (v.x, v.y) for v in face.bounding_poly.vertices]\n",
        "        print('Face surprised:', likelihood.name)\n",
        "        print('Face bounds:', \",\".join(vertices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euVpHHPSfjYO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}